---
title: "Regression estimator"
subtitle: "Blocks population"
output:
  pdf_document:
    keep_tex: no
    latex_engine: xelatex
    number_sections: yes
  html_document:
    toc: yes
  word_document: default
fontsize: 9pt
urlcolor: blue
header-includes:
- \usepackage{graphicx}
- \usepackage{epic}
- \usepackage{color}
- \usepackage{hyperref}
- \usepackage{multimedia}
- \newcommand{\ve}[1]{\mathbf{#1}}
- \newcommand{\pop}[1]{\mathcal{#1}}
- \newcommand{\samp}[1]{\mathcal{#1}}
- \newcommand{\subspace}[1]{\mathcal{#1}}
- \newcommand{\sv}[1]{\boldsymbol{#1}}
- \newcommand{\sm}[1]{\boldsymbol{#1}}
- \newcommand{\tr}[1]{{#1}^{\mkern-1.5mu\mathsf{T}}}
- \newcommand{\abs}[1]{\left\lvert ~{#1} ~\right\rvert}
- \newcommand{\size}[1]{\left\lvert {#1} \right\rvert}
- \newcommand{\norm}[1]{\left|\left|{#1}\right|\right|}
- \newcommand{\field}[1]{\mathbb{#1}}
- \newcommand{\Reals}{\field{R}}
- \newcommand{\Integers}{\field{Z}}
- \newcommand{\Naturals}{\field{N}}
- \newcommand{\Complex}{\field{C}}
- \newcommand{\Rationals}{\field{Q}}
- \newcommand{\widebar}[1]{\overline{#1}}
- \newcommand{\wig}[1]{\tilde{#1}}
- \newcommand{\bigwig}[1]{\widetilde{#1}}
- \newcommand{\leftgiven}{~\left\lvert~}
- \newcommand{\given}{~\vert~}
- \newcommand{\indep}{\bot\hspace{-.6em}\bot}
- \newcommand{\notindep}{\bot\hspace{-.6em}\bot\hspace{-0.75em}/\hspace{.4em}}
- \newcommand{\depend}{\Join}
- \newcommand{\notdepend}{\Join\hspace{-0.9 em}/\hspace{.4em}}
- \newcommand{\imply}{\Longrightarrow}
- \newcommand{\notimply}{\Longrightarrow \hspace{-1.5em}/ \hspace{0.8em}}
- \newcommand*{\intersect}{\cap}
- \newcommand*{\union}{\cup}
- \DeclareMathOperator*{\argmin}{arg\,min}
- \DeclareMathOperator*{\argmax}{arg\,max}
- \DeclareMathOperator*{\Ave}{Ave\,}
- \newcommand{\code}[1]{\texttt{#1}}
- \newcommand*{\Rnsp}{\textsf{R}}
- \newcommand*{\R}{\textsf{R}$~$}
- \newcommand*{\loonnsp}{\textsf{loon}}
- \newcommand*{\loon}{\textsf{loon}$~$}
- \newcommand*{\Pythonnsp}{\textsf{Python}}
- \newcommand*{\Python}{\textsf{Python}$~$}
- \newcommand*{\Tclnsp}{\textsf{Tcl}}
- \newcommand*{\Tcl}{\textsf{Tcl}$~$}
- \newcommand{\pkg}[1]{\textsf{#1}}
- \newcommand{\pkgsp}[1]{\textsf{#1}$~$}
- \newcommand{\lpart}[1]{\textsf{#1}}
- \newcommand{\lpartsp}[1]{\textsf{#1}$~$}
- \newcommand{\togglepause}{\pause}
- \newcommand{\suchthat}{~:~}
- \newcommand{\st}{~:~} 
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center", 
                      fig.width = 7, 
                      fig.height = 6,
                      out.height = "40%")
set.seed(12314159)
library(loon.data)
data("blocks", package = "loon.data")

codeDirectory <- "./R"
imageDirectory <- "./img"
dataDirectory <- "./data"
path_concat <- function(path1, path2, sep="/") paste(path1, path2, sep = sep)
```

---


Consider again the `blocks` data, as our study population $\pop{P}_{Study}$ consisting of $N = 100$ blocks labelled $u = 1, 2, 3, \ldots, 100$.

The blocks are of of uniform thickness and density (all blocks were cut from the same opaque plastic sheet of about 5mm thickness), but have different convex shapes such as shown below:


Recall that data on this population of 100 blocks are available as an \R data set `blocks` available in the package \pkg{loon.data}  and is loaded as follows:


Recall also the function `getAttributes()` from class

```{r}
getAttributes <- function(samples,     # list of samples (as units)
                          pop,         # population data frame 
                          variates,    # variates if needed by popAttr 
                          popAttr      # the pop attribute function
                             ) 
  {# Easy if variates supplied
   if (!missing(variates)) { # lapply forces a list
     lapply(samples, 
            FUN = function(samp) {popAttr(pop[samp, variates])} )
   } else {
     # variates are missing
     # Get number of dimensions of pop
     ndims <- length(dim(pop))
     if (!(ndims >= 2)) { # can only have a single index
       lapply(samples,
              FUN = function(samp) {popAttr(pop[samp])})
     } else { # first dimension identifies units in pop
       lapply(samples,
              FUN = function(samp) {popAttr(pop[samp, ])}) # note comma
     }
   }
}
```

---

**46 marks**

In this question, you will use `getAttributes()` to reproduce some of the results earlier obtained for various sampling plans and some new attributes.

Recall that `getAttributes()` requires its `samples` argument to be a `list` of the samples where each sample is a vector of indices identifying the units (rows) to appear in that sample.

Also, unlike `getAttributeVals()`, `getAttributes()` **returns** a `list` of the attribute evaluated on the samples.  If for example, the attribute is a single numerical value such as the average of some variate over the sample, these values will appear as individual elements in a `list`.  To coerce the list of numerical results to a vector (of numeric values), it will be necessary to call `as.numeric()` on the list returned by `getAttributes()`.


a. **(7 marks)** Different sampling plans.  In this question, lists of samples from the three different sampling plans you have seen before (simple random sampling, judgment sampling, stratified random sampling).  To make it easy to reproduce these plans, we first 

    ```{r}
   set.seed(1234567)
    ```

   All sampling plans will produce samples of size `n = 10` blocks from the $N =$ `nrow(blocks) =` `r (N <- nrow(blocks))` in the population of `blocks`. That means there are `r choose(N, 10)` different samples of size 10 available.

   i. *(2 marks)*  Simple Random Sampling.  Construct a collection of 1000 samples of size 10 by simple random sampling.  Save the list of samples as the variable `srsSamples` as follows
   
    ```{r}
      set.seed(1234567)
      N <- nrow(blocks)
      n <- 10
      nSamples <- 1000
      # Complete the following
      srsSamples <- lapply(1:nSamples,
                           FUN = function(i) {samp <- 
                             sample(N, n, replace=FALSE)})
    ```
   
      Show your result as the first two samples.
      
    ```{r}
      head(srsSamples, 2)
    ```
      
   ii. *(2 marks)*  Judgment Sampling.  Recall that there were judgment samples selected by students who had complete access to all 100 blocks.  These are also available from `loon.data` as
   
    ```{r}
       data("judgment", package = "loon.data")
    ```
      
       The sample indices now appear as the values of variates identifying the order in which each student selected the blocks.  These must be turned into a list of numeric vectors containing the indices for each sample. You must complete the following:
      
    ```{r}
       data("judgment", package = "loon.data")
       judgmentWeight <- judgment[c(2:11)]
       judgmentIndex <- dim(judgment)[1]
       judgmentSamples <- lapply(1:judgmentIndex,
                                 FUN = function(i){
                                   samp <- as.numeric(judgmentWeight[i[1],])})
    ```
       
       Again show your result as the first two judgment samples.
      
    ```{r}
       head(judgmentSamples, 2)
    ```
     
   iii. *(3 marks)*  Stratified random sampling.  Recall that two groups `A` and `B` had been identified in the population of `blocks`.  The stratified random sampling plan is to select 5 blocks at random from each group.  While there are many fewer possible stratified samples like this, there are still `r choose(50, 5)^2` to choose from.  
   
        Here you need to construct a list of 1000 such samples using this stratified random sampling plan and store the result on `stratSamples`.  
        
        To begin, first construct the two strata each containing the indices of that group.
        That is, complete
        
    ```{r}
        units <- 1:N
        stratumA <- blocks[blocks$group == "A",]$id
        stratumB <- blocks[blocks$group == "B",]$id
    ```
        
        Then use these to produce `stratSamples`.
        
    ```{r}
        stratSamples <- lapply(1:nSamples, 
                               FUN = function(i){
                               sampA <- sample(stratumA, 5, replace = FALSE)
                               sampB <- sample(stratumB, 5, replace = FALSE)
                               c(sampA,sampB)})
    ```
        
        Finally show the first two samples as
        
    ```{r}
        head(stratSamples, 2)
    ```
       
\newpage

b. **(6 marks)** Sampling distributions.  Now you will use `getAttributes()` to determine the sampling distribution of the average weight for each plan.  

   i. *(2 marks)* Simple random sampling.  Using the function `getAttributes()` find the average weight for each sample in `srsSamples` and save the results as a numeric vector on `srsAverageWts`.
   
      Show your code in constructing `srsAverageWts` and its first two values as
    ```{r}
      population <- blocks
      samples <- srsSamples
      variates <- "weight"
      attributeName <- "Average"
      PopAverage <- mean
      srsAverageWts <- as.numeric(getAttributes(samples = samples, # as a list
                                    pop = population,
                                    variates = variates,
                                    popAttr = PopAverage ))
      head(srsAverageWts, 2)
    ```

   ii. *(2 marks)* Judgment sampling.  Using the function `getAttributes()` find the average weight for each sample in `judgmentSamples` and save the results as a numeric vector on `judgmentAverageWts`.
   
       Show your code in constructing `judgmentAverageWts` and its first two values as
    ```{r}
       population <- blocks
       samples <- judgmentSamples
       variates <- "weight"
       attributeName <- "Average"
       PopAverage <- mean
       judgmentAverageWts <- as.numeric(getAttributes(samples = samples, 
                                    pop = population,
                                    variates = variates,
                                    popAttr = PopAverage ))
       head(judgmentAverageWts, 2)
    ```

   iii. *(2 marks)* Stratified random sampling.  Using the function `getAttributes()` find the average weight for each sample in `stratSamples` and save the results as a numeric vector on `stratAverageWts`.
   
        Show your code in constructing `stratAverageWts` and its first two values as
    ```{r}
        population <- blocks
        samples <- stratSamples
        variates <- "weight"
        attributeName <- "Average"
        PopAverage <- mean
        stratAverageWts <- as.numeric(getAttributes(samples = samples, 
                                    pop = population,
                                    variates = variates,
                                    popAttr = PopAverage ))
        head(stratAverageWts, 2)
    ```
\newpage

c. **(11 marks)** The perimeters of each block is also available.  Physical science informs us that the weight of a block (even these flat convex shaped blocks) is not going to be determined by its perimeter. But it should be related.

   i. *(1 mark)* What is the correlation between `weight` and `perimeter` in the `blocks` population?
    ```{r}
      (correlation <- {with(blocks, cor(x = blocks$weight, 
                                        y = blocks$perimeter))})
    ```
   
   
   ii. *(1 mark)* What is the Spearman's rho correlation between `weight` and `perimeter` in the `blocks` population?
    ```{r}
       (correlationS <- {with(blocks, cor(x = blocks$weight,
                                          y = blocks$perimeter,
                                          method = "spearman",))})
    ```
   
   iii. *(4 marks)* Fit `weight` as a straight line function of `perimeter` on the population.  Show a summary of the fit.
   
    ```{r}
        fit <- lm(weight ~ perimeter, data=blocks)
        summary(fit)
    ```
   
        What do you conclude about the fit?
Ans)    The Multiple R-squared and Adjusted R-squared value is around 0.81, 
        which indicated there is a large positive linear association between
        the weight and perimeter. We can assume from this that when perimeter
        increases the weight will decrease and vice versa. The p-value is 
        very low (very close to 0) which indicates that they have strong 
        correlations. The null hypothesis is rejected because the p-value is
        not equal to 0.


        
        
        
   iv. *(5 marks)* Produce a (nicely labelled) scatterplot of the pairs (`perimeter`, `weight`) for the population.  
   
       - Add the least squares fit of the conditional mean function $E(Y) = \mu(x)$ where $Y$ corresponds to `weight` and $x$ to `perimeter`.
       - the $95\%$ confidence intervals for $\mu(x)$
       - try to fit everything into the display by judicious choice of `xlim` and/or `ylim`
       - add a legend to distinguish the fitted $\mu(x)$ from its confidence interval
       
    ```{r, echo = TRUE}
    
       with(blocks,
       {plot(blocks$perimeter, blocks$weight, pch = 19, col = "steelblue",
             xlab = "perimeter", ylab = "weight", ylim = c(0,100))
       abline(coef(fit), col = "steelblue", lwd = 3)
       title(main = "Level 0.95 interval")
        })
    
       newValues <- data.frame(perimeter = seq(0, max(blocks$perimeter),
                                               length.out = 100))
       confIntervals <- predict(fit, newdata = newValues,
       interval = "confidence", level = 0.95)
       lowerConf <- confIntervals[, "lwr"]
       upperConf <- confIntervals[, "upr"]
       with(newValues,
       lines(perimeter, lowerConf, col = "firebrick", lty = 2, lwd = 3))
       with(newValues,
       lines(perimeter, upperConf, col = "firebrick", lty = 2, lwd = 3))
       legend("topleft",  bty = "n",
       legend = c(expression(paste("The fit: ", widehat(mu),(x))), 
                  expression(paste("Confidence interval for: ", mu(x))),
                  ""),
       col = c("steelblue", "firebrick", "white"), 
       lty = c(1, 2, 3), lwd = 3)
    ```
       

\newpage

d. **(2 marks)** Suppose that we know that the average perimeter of the population is $\widebar{X} =$ `r mean(blocks$perimeter)`.  We might use this to get a different estimate of the population average weight by using a line fitted to our sample and estimating the average weight to be the **predicted** weight when the perimeter is $\widebar{X}$ from the line fitted on the sample.  

    That is, we could estimate the population average weight by 
    \[ \widehat{\alpha}_{\samp{S}} + \widehat{\beta}_{\samp{S}} \times \widebar{X} \]
    where $\widehat{\alpha}_{\samp{S}}$ and $\widehat{\beta}_{\samp{S}}$ are the intercept and slope estimates from the line fitted to the `weight` and `perimeter` values from the **sample** $\samp{S}$ and $\widebar{X} =$ `r mean(blocks$perimeter)` is the **population** average perimeter.
    
    Let's do that.
    
    Write a function corresponding to the population attribute that returns the predicted value of the fitted straight line at $x = \widebar{X} =$ `r mean(blocks$perimeter)`.
       
    That is, complete the following function as a population attribute (to be used as `popAttr` in `getAttributes`):
       
    ```{r}
    regEstimate <- function(samp) {fit1 = lm(weight ~ perimeter, data=samp)
    predict(fit1,data.frame(perimeter=c(mean(blocks$perimeter))))}
    ```
       
    Show your code and the value on the population, namely
    ```{r}
    (regEstimate(blocks))
    ```
       
    
\newpage

e. **(6 marks)** The sampling distributions of the **regression estimator** `regEstimate()`.  Knowing the population average perimeter, we can now look at the sampling distribution of the attribute `regEstimate()` under the various plans.

   i. *(2 marks)* Simple random sampling.  Using `getAttributes()` produce the numeric vector containing the `regEstimate()` for samples in `srsSamples`
   
      That is complete
      
    ```{r}
      population <- blocks
      samples <- srsSamples
      variates <- c("weight","perimeter")
      attributeName <- "regEstimate"
      PopAverage <- regEstimate
      srsRegEstimates <- as.numeric(getAttributes(samples = samples, # as a list
                                    pop = population,
                                    variates = variates,
                                    popAttr = PopAverage ))
      head(srsRegEstimates, 2)
     
    ```
      
      and show its first 2 values

 

   ii. *(2 marks)* Judgment sampling.  Using `getAttributes()` produce the numeric vector containing the `regEstimate()` for samples in `judgmentSamples`
   
       That is complete
       and show its first 2 values
      
    ```{r}
       population <- blocks
       samples <- judgmentSamples
       variates <- c("weight","perimeter")
       attributeName <- "regEstimate"
       PopAverage <- regEstimate
       judgmentRegEstimates <- as.numeric(getAttributes(samples = samples,
                                    pop = population,
                                    variates = variates,
                                    popAttr = PopAverage ))
       head(judgmentRegEstimates, 2)
    ```
      


   iii. *(2 marks)* Stratified random sampling.  Using `getAttributes()` produce the numeric vector containing the `regEstimate()` for samples in `stratSamples`
   
        That is complete
        and show its first 2 values
      
    ```{r}
        population <- blocks
        samples <- stratSamples
        variates <- c("weight","perimeter")
        attributeName <- "regEstimate"
        PopAverage <- regEstimate
        stratRegEstimates <- as.numeric(getAttributes(samples = samples, 
                                    pop = population,
                                    variates = variates,
                                    popAttr = PopAverage ))
        head(stratRegEstimates, 2)
    ```
        
\newpage

f. **(12 marks)** Comparing sampling distributions. Here the effect of using a regression estimate will be investigated by comparing histograms within each sampling plan. 

   The header of each \R{} code block to produce a pair of histograms will be `{r, fig.height = 7, fig.width = 6, out.height = "100%"}`  and each block should include

   ```{r, eval = FALSE, fig.height = 7, fig.width = 6, out.height = "100%"}
   xlim <- extendrange(blocks$weight)
   popAveWeight <- mean(blocks$weight)
   savePar <- par(mfrow = c(2,1))
   # do the plotting of the two histograms here with the above xlim
   # in each case and a vertical red line at popAveWeight
   # hist(aves ... etc,
          main = "Whatever plan: Sample averages",
          xlim = xlim)
   # hist(regEstimates ... etc,
          main = "Whatever plan: Regression estimates",
          xlim = xlim)
   # then reset the paraneters
   par(savePar)
   ```
   
   Each display will then consist of two suitably labelled histograms (in separate plots and one above the other),  first of the values of the average weights for those samples and second of the `regEstimates` for the same samples.  
   
   A vertical "red" line at `v = popAveWeight` is to appear on every histogram.
  
   
   i. *(4 marks)* Simple random sampling.  Draw two suitably labelled histograms in separate plots, one above the other, first of the values `srsAverageWts` and then of `srsRegEstimates`.
    ```{r, echo = TRUE, fig.height = 7, fig.width = 6, out.height = "100%"}
      xlim <- extendrange(blocks$weight)
      popAveWeight <- mean(blocks$weight)
      savePar <- par(mfrow = c(2,1))
      # do the plotting of the two histograms here with the above xlim
      # in each case and a vertical red line at popAveWeight
      hist(srsAverageWts, main = "Simple random sampling: Sample averages",
         xlim = xlim, xlab = "Srs Average Weights")
      abline(v=popAveWeight, col = "red", lty = 2, lwd = 2)
      hist(srsRegEstimates,
          main = "Simple random sampling: Regression estimates",
          xlim = xlim, xlab = "Srs Regression estimates")
      abline(v=popAveWeight, col = "red", lty = 2, lwd = 2)
      par(savePar)
    ```
     
   ii. *(4 marks)* Judgment sampling.  Draw two suitably labelled histograms in separate plots, one above the other, first of the values `judgmentAverageWts` and then of `judgmentRegEstimates`.
   
   
    ```{r, echo = TRUE, fig.height = 7, fig.width = 6, out.height = "100%"}
       xlim <- extendrange(blocks$weight)
       popAveWeight <- mean(blocks$weight)
       savePar <- par(mfrow = c(2,1))
       # do the plotting of the two histograms here with the above xlim
       # in each case and a vertical red line at popAveWeight
       hist(judgmentAverageWts, main = "Judgment sampling: Sample averages",
         xlim = xlim, xlab = "Judgment Average Weights")
       abline(v=popAveWeight, col = "red", lty = 2, lwd = 2)
       hist(judgmentRegEstimates,
          main = "Judgment sampling: Regression estimates",
          xlim = xlim, xlab = "Judgment Regression estimates")
       abline(v=popAveWeight, col = "red", lty = 2, lwd = 2)
       par(savePar)
    ```
   iii. *(4 marks)* Stratified random sampling.  Draw two suitably labelled histograms in separate plots, one above the other, first of the values `stratAverageWts` and then of `stratRegEstimates`.
   
    ```{r, echo = TRUE, fig.height = 7, fig.width = 6, out.height = "100%"}
        xlim <- extendrange(blocks$weight)
        popAveWeight <- mean(blocks$weight)
        savePar <- par(mfrow = c(2,1))
        # do the plotting of the two histograms here with the above xlim
        # in each case and a vertical red line at popAveWeight
        hist(stratAverageWts, 
             main = "Stratified random sampling: Sample averages",
         xlim = xlim, xlab = "Stratified Average Weights")
        abline(v=popAveWeight, col = "red", lty = 2, lwd = 2)
        hist(stratRegEstimates,
          main = "Stratified random sampling: Regression estimates",
          xlim = xlim, xlab = "Stratified Regression estimates")
        abline(v=popAveWeight, col = "red", lty = 2, lwd = 2)
        par(savePar)
    ```
   
\newpage

g. **(2 marks)** In light of the foregoing analysis, comment on the meaning  of the phrase "All models are wrong, but some are useful" and its relevance in this context.

Ans) "All models are wrong, but some are useful", every model is wrong because
it simplifies things and ignores important parameters that could be used in
the model but this simplification can be useful, because with the help of 
it, we can predict and understand different processes.
Here, we produce simple random sampling, judgement sampling and stratified 
random sampling and use it to find attributes, regression estimator, etc. 
These models might not be very accurate as compared to the target sample we 
would like to produce, but they do have some sort of usefulness in the results 
they produce.



  